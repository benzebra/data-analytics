{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#import ordinal ecnoder\n",
    "from sklearn.preprocessing import OrdinalEncoder,LabelEncoder\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torcheval.metrics as tm\n",
    "\n",
    "## TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "## Sklearn\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score,f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "## Saving, Loading and Plotting\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# keep for TabTransformer computation (CUDA memory management)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: rf\n",
      "Preprocessing: pca\n",
      "Overfit: False\n",
      "Machine Learning: True\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "FILENAME = \"train_dataset.csv\"\n",
    "\n",
    "clf = 'tf'         # 'rf', 'svm', 'knn', 'ff', 'tf' or 'tb'\n",
    "pre = 'pca'         # 'pca', 'lda', 'std'\n",
    "overfit = True      # True or False\n",
    "\n",
    "ml = False           # DON'T CHANGE (True for Machine Learning, False for Deep Learning)\n",
    "\n",
    "if ml:\n",
    "    overfit = False\n",
    "\n",
    "if clf == 'ff' or clf == 'tb' or clf == 'tf':\n",
    "    overfit = True\n",
    "    ml = False\n",
    "\n",
    "print(f\"Classifier: {clf}\")\n",
    "print(f\"Preprocessing: {pre}\")\n",
    "print(f\"Overfit: {overfit}\")\n",
    "print(f\"Machine Learning: {ml}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ml:\n",
    "    # look for GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed Fixing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "    Args:\n",
    "        seed: the seed to use. \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ml:\n",
    "    # Define the Data Layer        \n",
    "    class MyDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            \n",
    "            self.X = torch.FloatTensor(X)\n",
    "            self.y = torch.LongTensor(y)\n",
    "            \n",
    "            self.num_features = X.shape[1]\n",
    "            self.num_classes = len(np.unique(y))\n",
    "        \n",
    "\n",
    "        def __len__(self):\n",
    "            return self.X.shape[0]\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'ff':\n",
    "    # Architecture\n",
    "    class FeedForwardPlus(nn.Module):\n",
    "        def __init__(self, input_size, num_classes, hidden_size, depth=1, batch_norm=False, drop=0):\n",
    "            super(FeedForwardPlus, self).__init__()\n",
    "            model = []\n",
    "            model += [nn.Linear(input_size, hidden_size)]\n",
    "            if batch_norm:\n",
    "                model += [nn.BatchNorm1d(hidden_size)]\n",
    "            model += [nn.ReLU()]\n",
    "\n",
    "            block = [\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "\n",
    "            block_batch_norm = [\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "\n",
    "            block_dropout = [\n",
    "                nn.Dropout(drop),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "\n",
    "            for i in range(depth):\n",
    "                if not batch_norm and drop == 0:\n",
    "                    model += block\n",
    "                elif batch_norm and drop == 0:\n",
    "                    model += block_batch_norm\n",
    "                elif drop > 0 and not batch_norm:\n",
    "                    model += block_dropout\n",
    "            \n",
    "            self.model = nn.Sequential(*model)\n",
    "            \n",
    "            self.output = nn.Linear(hidden_size, num_classes)\n",
    "            \n",
    "\n",
    "        def forward(self, x):\n",
    "            h = self.model(x)\n",
    "            out = self.output(h)\n",
    "            return out\n",
    "\n",
    "        \n",
    "    def test_model(model, data_loader, device):\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_test = []\n",
    "        \n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            y_pred += model(data)\n",
    "            #print(y_pred)\n",
    "            y_test += targets\n",
    "            #print(targets)\n",
    "        \n",
    "        y_test = torch.stack(y_test).squeeze()\n",
    "        y_pred = torch.stack(y_pred).squeeze()\n",
    "        y_pred_c = y_pred.argmax(dim=1, keepdim=True).squeeze()\n",
    "        \n",
    "        return y_test, y_pred_c, y_pred\n",
    "\n",
    "    def train_model(model, criterion, optimizer, epoch, scheduler, train_loader, val_loader, device, writer, log_name=\"model\"):\n",
    "        n_iter = 0\n",
    "        best_valid_loss = float('inf')\n",
    "        for epoch in range(epoch):\n",
    "            model.train()\n",
    "            \n",
    "            for data, targets in train_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = model(data)\n",
    "\n",
    "                # Compute Loss\n",
    "                loss = criterion(y_pred, targets)\n",
    "            \n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                n_iter += 1\n",
    "            \n",
    "            labels, _, y_pred = test_model(model, val_loader, device)\n",
    "            loss_val = criterion(y_pred, labels)\n",
    "            \n",
    "            \n",
    "            f1 = tm.MulticlassF1Score(num_classes=labels.max().item() + 1)\n",
    "            f1.update(y_pred, labels)\n",
    "            writer.add_scalar(log_name, f1.compute().item(), epoch)\n",
    "            \n",
    "        \n",
    "            \n",
    "            # Save best model\n",
    "            if loss_val.item() < best_valid_loss:\n",
    "                best_valid_loss = loss_val.item()\n",
    "                if not os.path.exists('models'):\n",
    "                    os.makedirs('models')\n",
    "                torch.save(model.state_dict(), 'models/'+log_name)\n",
    "            \n",
    "            \n",
    "            \n",
    "            (log_name, scheduler.get_last_lr()[0], epoch)\n",
    "            \n",
    "            scheduler.step()\n",
    "        return model, best_valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'tb':\n",
    "    class TabNet(torch.nn.Module):\n",
    "        '''\n",
    "        Wrapper class for TabNetClassifier\n",
    "        '''\n",
    "        def __init__(self, n_d,\n",
    "                    n_a,\n",
    "                    n_steps,\n",
    "                    gamma,\n",
    "                    optimizer_fn,\n",
    "                    n_independent,\n",
    "                    n_shared,\n",
    "                    epsilon,\n",
    "                    seed,\n",
    "                    lambda_sparse,\n",
    "                    clip_value,\n",
    "                    momentum,\n",
    "                    optimizer_params,\n",
    "                    scheduler_params,\n",
    "                    mask_type,\n",
    "                    scheduler_fn,\n",
    "                    device_name,\n",
    "                    output_dim,\n",
    "                    batch_size,\n",
    "                    num_epochs,\n",
    "                    unsupervised_model,\n",
    "                    verbose=0):\n",
    "            super(TabNet, self).__init__()\n",
    "\n",
    "            self.batch_size = batch_size\n",
    "            self.num_epochs = num_epochs\n",
    "            self.unsupervised_model = unsupervised_model\n",
    "            self.network = TabNetClassifier(n_d=n_d,\n",
    "                                            n_a=n_a,\n",
    "                                            n_steps=n_steps,\n",
    "                                            gamma=gamma,\n",
    "                                            optimizer_fn=optimizer_fn,\n",
    "                                            n_independent=n_independent,\n",
    "                                            n_shared=n_shared,\n",
    "                                            epsilon=epsilon,\n",
    "                                            seed=seed,\n",
    "                                            lambda_sparse=lambda_sparse,\n",
    "                                            clip_value=clip_value,\n",
    "                                            momentum=momentum,\n",
    "                                            optimizer_params=optimizer_params,\n",
    "                                            scheduler_params=scheduler_params,\n",
    "                                            mask_type=mask_type,\n",
    "                                            scheduler_fn=scheduler_fn,\n",
    "                                            device_name=device_name,\n",
    "                                            output_dim=output_dim,\n",
    "                                            verbose=verbose)\n",
    "        \n",
    "        def fit_model(self, X_train, y_train, X_val, y_val, criterion):\n",
    "            self.network.fit(X_train=X_train, \n",
    "                            y_train=y_train, \n",
    "                            eval_set=[(X_train,y_train),(X_val, y_val)], \n",
    "                            eval_metric=['balanced_accuracy'], \n",
    "                            patience=10, \n",
    "                            batch_size=self.batch_size, \n",
    "                            virtual_batch_size=128, \n",
    "                            num_workers=0, \n",
    "                            drop_last=True, \n",
    "                            max_epochs=self.num_epochs, \n",
    "                            loss_fn=criterion, \n",
    "                            from_unsupervised=self.unsupervised_model)\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.network.predict(X)\n",
    "        \n",
    "        def explain(self, X):\n",
    "            return self.network.explain(X)\n",
    "        \n",
    "        def feature_importances(self):\n",
    "            return self.network.feature_importances_\n",
    "\n",
    "    def get_unsupervised_model(n_d_a,n_step,n_independent,n_shared,gamma,lr):\n",
    "        tabnet_params = dict(n_d=n_d_a, \n",
    "                            n_a=n_d_a,\n",
    "                            n_steps=n_step,\n",
    "                            gamma=gamma,\n",
    "                            n_independent=n_independent,\n",
    "                            n_shared=n_shared,\n",
    "                            lambda_sparse=1e-3,\n",
    "                            optimizer_fn=torch.optim.AdamW, \n",
    "                            optimizer_params=dict(lr=lr),\n",
    "                            mask_type=\"sparsemax\",\n",
    "                            verbose=0\n",
    "                            )\n",
    "        unsupervised_model = TabNetPretrainer(**tabnet_params)\n",
    "        return unsupervised_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabTransformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'tf':\n",
    "    class TabTransformer(torch.nn.Module):\n",
    "        def __init__(self, num_features, num_classes, dim_embedding=8, num_heads=2, num_layers=2):\n",
    "            super(TabTransformer, self).__init__()\n",
    "            self.embedding = torch.nn.Linear(num_features, dim_embedding)\n",
    "            encoder_layer = torch.nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True)\n",
    "            self.transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "            self.classifier = torch.nn.Linear(dim_embedding, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x)\n",
    "            x = x.unsqueeze(1)  # Adding a sequence length dimension\n",
    "            x = self.transformer(x)\n",
    "            x = torch.mean(x, dim=1)  # Pooling\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "    \n",
    "    def train_model(model, criterion, optimizer, epochs, data_loader, val_loader, device, scheduler, patience):\n",
    "        n_iter = 0\n",
    "        best_model = None\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_since_last_improvement = 0\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "\n",
    "            start_epoch = time.time()\n",
    "\n",
    "            loss_train = 0\n",
    "            for data, targets in data_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                n_iter += 1\n",
    "                loss_train += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            loss_train /= len(data_loader)\n",
    "\n",
    "            # Compute Val Loss\n",
    "            val_loss,_,_ = test_model(model, criterion, val_loader)\n",
    "\n",
    "            loss_history.append(loss_train)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                epochs_since_last_improvement = 0\n",
    "            elif epochs_since_last_improvement >= patience:\n",
    "                break\n",
    "            else:\n",
    "                epochs_since_last_improvement += 1\n",
    "\n",
    "            print('Epoch [{}/{}] - {:.2f} seconds - train_loss: {:.6f} - val_loss: {:.6f} - patience: {}'.format(epoch+1,\n",
    "                epochs, time.time() - start_epoch, loss_train, val_loss, epochs_since_last_improvement), end='\\r')\n",
    "\n",
    "        print('\\nTraining ended after {:.2f} seconds - Best val_loss: {:.6f}'.format(time.time() - start, best_val_loss))\n",
    "\n",
    "        return best_model, loss_history, val_loss_history\n",
    "\n",
    "\n",
    "    def test_model(model, criterion, loader):\n",
    "        model.eval()\n",
    "        y_pred = torch.tensor([],requires_grad=True).to(device)\n",
    "        y_true = torch.tensor([],requires_grad=True).to(device)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for data, targets in loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            preds = model(data)\n",
    "            loss = criterion(preds, targets.long())\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.cat((y_pred, preds.squeeze()))\n",
    "            y_true = torch.cat((y_true, targets.detach()))\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        return avg_loss, y_pred.squeeze(), y_true.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"train_dataset.csv\"\n",
    "# 1, 2 e 3\n",
    "df = pd.read_csv(FILENAME, sep=\",\", low_memory=False)\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=[\"label\"])\n",
    "\n",
    "# 4\n",
    "df[\"src_bytes\"] = df[\"src_bytes\"].replace(\"0.0.0.0\", np.nan).astype(float)\n",
    "mean_src_bytes = df[\"src_bytes\"].mean()\n",
    "df[\"src_bytes\"] = df[\"src_bytes\"].fillna(mean_src_bytes)\n",
    "\n",
    "# 5\n",
    "df.astype({'src_bytes': 'int64', 'ts': 'datetime64[ms]', 'dns_AA': 'bool', 'dns_RD': 'bool', 'dns_RA': 'bool', 'dns_rejected': 'bool', 'ssl_resumed': 'bool', 'ssl_established': 'bool', 'weird_notice': 'bool'}).dtypes\n",
    "\n",
    "# 6\n",
    "y = df[\"type\"]\n",
    "df = df.drop(columns=[\"type\"])\n",
    "\n",
    "# 7\n",
    "oe = OrdinalEncoder()\n",
    "df_oe = oe.fit_transform(df.select_dtypes(include=['object']))\n",
    "df.loc[:, df.select_dtypes(include=['object']).columns] = df_oe\n",
    "X = df.to_numpy()\n",
    "\n",
    "# 8\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "if not ml:\n",
    "    # Separate train, val, and test\n",
    "    indices = np.arange(X.shape[0])\n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.4, stratify=y[temp_idx], random_state=seed)\n",
    "\n",
    "    X_train = X[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[val_idx, :]\n",
    "    y_val = y[val_idx]\n",
    "    X_test = X[test_idx, :]\n",
    "    y_test = y[test_idx]\n",
    "    \n",
    "else:\n",
    "    indices = np.arange(X.shape[0])\n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=0.2, stratify=y, random_state=seed)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.4, stratify=y[temp_idx], random_state=seed)\n",
    "\n",
    "    fold = np.zeros(X.shape[0])\n",
    "    fold[train_idx] = -1\n",
    "    fold[val_idx] = 0\n",
    "\n",
    "    ps = PredefinedSplit(fold)\n",
    "    ps.get_n_splits()\n",
    "\n",
    "    X_train = X[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[val_idx, :]\n",
    "    y_val = y[val_idx]\n",
    "    X_test = X[test_idx, :]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "### Scaling & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA\n",
      "pca saved\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "if overfit:\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "if pre == 'std':\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    #save scaler\n",
    "    with open('scaler.save', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(\"Applying Standard Scaler\")\n",
    "    \n",
    "if pre == 'pca' or pre == 'lda':\n",
    "    if pre == 'pca':\n",
    "        # scaler.fit(X)\n",
    "        # X = scaler.transform(X)\n",
    "        print(\"Applying PCA\")\n",
    "        pre = decomposition.PCA(n_components='mle', svd_solver='full')\n",
    "        pre.fit(X_train)\n",
    "        X_train = pre.transform(X_train)\n",
    "        X_val = pre.transform(X_val)\n",
    "        # pre.fit(X)\n",
    "        \n",
    "        #save pca\n",
    "        # with open('pca.save', 'wb') as f:\n",
    "        #     pickle.dump(pre, f)\n",
    "        print('pca saved')\n",
    "        # X = pre.transform(X)\n",
    "        if overfit:\n",
    "            X_test = pre.transform(X_test)\n",
    "    elif pre == 'lda':\n",
    "        scaler.fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        print(\"Applying LDA\")\n",
    "        pre = LinearDiscriminantAnalysis()\n",
    "        # pre.fit(X_train, y_train)\n",
    "        # pre.transform(X_train)\n",
    "        # pre.transform(X_val)\n",
    "        # save lda\n",
    "        pre.fit(X, y)\n",
    "        with open('lda.save', 'wb') as f:\n",
    "            pickle.dump(pre, f)\n",
    "        # X = pre.transform(X)\n",
    "        if overfit:\n",
    "            X_test = pre.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset/Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ml:\n",
    "    if clf == 'ff':\n",
    "        train_dataset = MyDataset(X_train, y_train)\n",
    "        val_dataset = MyDataset(X_val, y_val)\n",
    "        test_dataset = MyDataset(X_test, y_test)\n",
    "    elif clf == 'tf' or clf == 'tb':\n",
    "        num_feature = X_train.shape[1]\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor), batch_size=y_val.shape[0], shuffle=False)\n",
    "        test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor), batch_size=y_test.shape[0], shuffle=False)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "### Model Selection with best hp configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50\n",
      "building tree 2 of 50\n",
      "building tree 3 of 50\n",
      "building tree 4 of 50\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  31 tasks      | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n"
     ]
    }
   ],
   "source": [
    "if clf == 'rf':\n",
    "    rf = RandomForestClassifier(n_estimators=50, criterion='gini', random_state=seed,verbose=3)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "   \n",
    "    # Save model\n",
    "    with open(\"models/rf/rf.save\", \"wb\") as f:\n",
    "        pickle.dump(rf, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'svm':\n",
    "    svm = SVC(C=1000,kernel='rbf',gamma='auto')\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    # Save model\n",
    "    with open(\"models/svm/svm.save\", \"wb\") as f:\n",
    "        pickle.dump(svm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'knn':\n",
    "    knn = KNeighborsClassifier(n_neighbors=2, p=1)\n",
    "    knn.fit(X, y)\n",
    "    with open(\"models/knn/knn.save\", \"wb\") as f:\n",
    "        pickle.dump(knn, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'ff':\n",
    "    start = time.time()\n",
    "    #hyperparameters\n",
    "    seed = 42\n",
    "    batch_sizes = [512]\n",
    "    hidden_sizes = [32] # 64\n",
    "    batch_norm_list = [False]\n",
    "    drop = 0\n",
    "    depths = [2]\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.01\n",
    "    gammas = [0.5]\n",
    "    step_size = int(num_epochs / 4)\n",
    "    hyperparameters = itertools.product(batch_sizes, hidden_sizes, depths, gammas, batch_norm_list)\n",
    "\n",
    "\n",
    "    #grid search loop\n",
    "    for batch_size, hidden_size, depth, gamma, batch_norm in hyperparameters:\n",
    "        fix_random(seed)\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        log_name = \"B\"+str(batch_size)+\"-dim\"+str(hidden_size)+\"-dp\"+str(depth)+\"-ep\"+str(num_epochs)+\"-lr\"+str(learning_rate)+\"-steplr\"+str(step_size)+\"-gamma\"+str(gamma)+\"-BN\"+str(batch_norm)+\"-drop\"+str(drop)\n",
    "        print(log_name, end=\", \")\n",
    "        \n",
    "        #start tensorboard\n",
    "        writer = SummaryWriter('runs/'+log_name)\n",
    "\n",
    "        # Create relative dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        #define architecture, loss and optimizer\n",
    "        model = FeedForwardPlus(train_dataset.num_features, train_dataset.num_classes, hidden_size, depth, batch_norm=batch_norm)\n",
    "        model.to(device)\n",
    "        \n",
    "\n",
    "        #train\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        model, best_valid_loss = train_model(model, criterion, optimizer, num_epochs, scheduler, train_loader, val_loader, device, writer, log_name)\n",
    "\n",
    "        writer.add_hparams({'hparam/bsize': batch_size, 'hparam/hidden size': hidden_size, 'hparam/depth':depth+2, 'hparam/scheduler': gamma,'hparam/batch norm': batch_norm}, {'best loss': best_valid_loss})\n",
    "        writer.flush()\n",
    "        \n",
    "        #save hyper parameters\n",
    "        with open(\"ff_hp.pkl\", \"wb\") as f:\n",
    "            pickle.dump({'batch_size': batch_size, 'hidden_size': hidden_size, 'depth': depth, 'gamma': gamma, 'batch_norm': batch_norm}, f)\n",
    "        \n",
    "        print(\"time elapsed:\", time.time() - start)\n",
    "    writer.close()\n",
    "    \n",
    "    #save the best model in a .save file with pickle\n",
    "    with open(\"ff.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    with open(\"ff.save\", \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'tb':\n",
    "    nums_epochs = [10]\n",
    "    batch_sizes = [256]\n",
    "    patience = [10]\n",
    "    n_d_a = [16]\n",
    "    n_shared = [8]\n",
    "    n_indipendents = [1] \n",
    "    n_steps = [9]\n",
    "    gamma = [1.0]\n",
    "    epsilon = [1e-15]\n",
    "    learning_rate = [0.01]\n",
    "    pretraining_ratio = [0.5]\n",
    "    momentum = [0.99]\n",
    "    hyperparameters = list(itertools.product(nums_epochs, batch_sizes, patience, n_d_a, n_indipendents, n_shared, n_steps, gamma, epsilon, learning_rate, pretraining_ratio, momentum))\n",
    "\n",
    "    current_iter = 0\n",
    "    best_acc = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "    for num_epochs, batch_size, patience_, n_d, n_i, n_s, n_steps_, gamma_, epsilon_, lr, pretraining_ratio_, moment in hyperparameters:\n",
    "        #print epoch\n",
    "        print(f'Hyperparameters: num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}')\n",
    "\n",
    "        unsupervised_model = get_unsupervised_model(n_d, n_steps_, n_i, n_s, gamma_, lr)\n",
    "            \n",
    "        unsupervised_model.fit(\n",
    "            X_train=X_train,\n",
    "            eval_set=[X_val],\n",
    "            max_epochs=num_epochs,\n",
    "            patience=patience_,\n",
    "            batch_size=batch_size,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            pretraining_ratio=pretraining_ratio_,\n",
    "        )\n",
    "\n",
    "        model = TabNet(n_d=n_d,\n",
    "                    n_a=n_d,\n",
    "                    n_steps=n_steps_,\n",
    "                    gamma=gamma_,\n",
    "                    optimizer_fn=torch.optim.AdamW,\n",
    "                    n_independent=n_i,\n",
    "                    n_shared=n_s,\n",
    "                    epsilon=epsilon_,\n",
    "                    seed=seed,\n",
    "                    lambda_sparse=1e-4,\n",
    "                    clip_value=1,\n",
    "                    momentum=moment,\n",
    "                    optimizer_params=dict(lr=lr),\n",
    "                    scheduler_params=dict(step_size=10, gamma=0.9),\n",
    "                    mask_type='sparsemax',\n",
    "                    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                    device_name=device,\n",
    "                    output_dim=len(np.unique(y_train)),\n",
    "                    batch_size=batch_size,\n",
    "                    num_epochs=num_epochs,\n",
    "                    unsupervised_model=None,\n",
    "                    verbose=0)\n",
    "        model.fit_model(X_train, y_train, X_val, y_val, criterion)    \n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_hyperparameters = f\"num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}\"\n",
    "        current_iter += 1\n",
    "    \n",
    "    #save best model with pickle\n",
    "    with open('models/tabnet/tb.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    #save best hyperparameters with pickle\n",
    "    with open('models/tabnet/tb_hp.pkl', 'wb') as f:\n",
    "        pickle.dump(best_hyperparameters, f)\n",
    "    \n",
    "    #save best model with pickle\n",
    "    with open('models/tabnet/tb.save', 'wb') as f:\n",
    "        pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'tf':\n",
    "    nums_epochs = [20]\n",
    "    batch_sizes = [512]\n",
    "    patience = [10]\n",
    "    dim_embedding = [32]\n",
    "    num_heads = [8]\n",
    "    num_layers = [16]\n",
    "    learning_rate = [0.001]\n",
    "    hyperparameters = list(itertools.product(nums_epochs, batch_sizes, patience, dim_embedding, num_heads, num_layers, learning_rate))\n",
    "    \n",
    "    #Initialize the model, loss, and optimizer\n",
    "    best_loss = float('inf')\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n",
    "    current_iter = 0\n",
    "    for epochs, batch_size, patience_, dim_embedding_, num_heads_, num_layers_, lr in hyperparameters:\n",
    "        train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, drop_last=True)\n",
    "\n",
    "        model = TabTransformer(num_feature, num_classes).to(torch.device('cuda'))\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "        model, loss_history, val_loss_history = train_model(model, criterion, optimizer, epochs, train_loader, val_loader, device, scheduler, patience_)\n",
    "        val_loss, y_pred, y_true = test_model(model, criterion, val_loader)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_hyperparameters = f\"epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}\"\n",
    "\n",
    "        print(f'Hyperparameters: epochs={epochs}, batch_size={batch_size}, patience={patience_}, dim_embedding={dim_embedding_}, num_heads={num_heads_}, num_layers={num_layers_}, lr={lr}')\n",
    "        print(f'Validation Loss: {val_loss}')\n",
    "\n",
    "        current_iter += 1\n",
    "    \n",
    "    #save best model with pickle\n",
    "    # with open('models/tabtransf/tf.pkl', 'wb') as f:\n",
    "    #     pickle.dump(best_model, f)\n",
    "    \n",
    "    # #save best model as .save with pickle\n",
    "    # with open('models/tabtransf/tf.save', 'wb') as f:\n",
    "    #     torch.save(best_model, f)\n",
    "\n",
    "    # #save best hyperparameters with pickle\n",
    "    # with open('models/tabtransf/tf_hp.pkl', 'wb') as f:\n",
    "    #     pickle.dump(best_hyperparameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/Evaluation\n",
    "\n",
    "### Model Saving + Best hp configuration saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf == 'ff':\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_sizes[0])\n",
    "    # Load the best model and evaluate it on the test set\n",
    "    with open(\"models/ffnn/ff.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    y_test, y_pred_c, y_pred = test_model(model, test_loader, device)\n",
    "\n",
    "    # print accuracy\n",
    "    print(tm.MulticlassAccuracy().update(y_pred_c, y_test).compute().item())\n",
    "\n",
    "    # print precision\n",
    "    print(tm.MulticlassPrecision(num_classes=y_test.max().item() + 1).update(y_pred, y_test).compute().item())\n",
    "\n",
    "    # Classification report with torcheval\n",
    "    f1 = tm.MulticlassF1Score(num_classes=y_test.max().item() + 1, )\n",
    "    f1.update(y_pred, y_test)\n",
    "    print(f1.compute().item())\n",
    "\n",
    "    #confusion matrix\n",
    "    conf_matrix = tm.MulticlassConfusionMatrix(num_classes=y_test.max().item() + 1)\n",
    "    conf_matrix.update(y_pred_c, y_test)\n",
    "\n",
    "    #use sklearn confusion matrix\n",
    "    ConfusionMatrixDisplay(conf_matrix.compute().cpu().numpy()).plot()\n",
    "\n",
    "    #compute classification report\n",
    "    print(classification_report(y_test.cpu().numpy(), y_pred_c.cpu().numpy()))\n",
    "\n",
    "    #save classification report as txt\n",
    "    with open('ffnn_classification_report.txt', 'w') as f:\n",
    "        f.write(classification_report(y_test.cpu().numpy(), y_pred_c.cpu().numpy()))\n",
    "\n",
    "    #compute balanced accuracy\n",
    "    print(\"balanced accuracy: \",balanced_accuracy_score(y_test.cpu().numpy(), y_pred_c.cpu().numpy()))\n",
    "    \n",
    "if clf=='tb':\n",
    "    #save best hp\n",
    "    with open('models/tabnet/tb_hp.txt', 'w') as f:\n",
    "        f.write(best_hyperparameters)\n",
    "        \n",
    "    print(f'Best model hyperparameters: {best_hyperparameters}')\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {acc}\\n')\n",
    "\n",
    "    #compute the classficication report\n",
    "    from sklearn.metrics import classification_report,balanced_accuracy_score\n",
    "\n",
    "    #print balanced accuracy\n",
    "    print(f'Balanced accuracy: {balanced_accuracy_score(y_test, y_pred)}\\n')\n",
    "\n",
    "    #f1 score\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred, average=\"weighted\")}\\n')\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #save it\n",
    "    with open('models/tabnet/tb_report.txt', 'w') as f:\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "    #compute the confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    \n",
    "if clf == 'tf':\n",
    "    test_loss, y_pred, y_true = test_model(best_model, criterion, test_loader)\n",
    "    y_pred = torch.argmax(y_pred, dim=1)\n",
    "    print(f'Best hyperparameters: {best_hyperparameters}')\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test Accuracy: {accuracy_score(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())}')\n",
    "    #print balanced accuracy\n",
    "    print(f'Balanced Accuracy: {balanced_accuracy_score(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())}')\n",
    "    #print f1 score\n",
    "    print(f'F1 Score: {f1_score(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average=\"weighted\")}')\n",
    "    #compute confusion matrix\n",
    "    cm = confusion_matrix(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    #print classification report\n",
    "    print(f'Classification Report:\\n {classification_report(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
